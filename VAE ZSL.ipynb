{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a preciate matrix-2d list\n",
    "predicate_file = \"predicate-matrix-binary.csv\"\n",
    "with open(predicate_file, \"r\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    predicate_matrix = []\n",
    "    for row in reader:\n",
    "        binary_digits = row[0].split(\" \")\n",
    "        predicate_matrix.append([int(value) for value in binary_digits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features_file, labels_file,slice, transform=None):\n",
    "        self.slice = slice\n",
    "        self.features_frame = pd.read_csv(features_file)\n",
    "        self.features_frame = self.features_frame.sample(frac=self.slice,random_state=42)\n",
    "        self.labels_frame = pd.read_csv(labels_file,header=None)\n",
    "        self.labels_frame = self.labels_frame.sample(frac=self.slice,random_state=42)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        features = self.features_frame.iloc[idx, :].values.astype('float32')\n",
    "        label = self.labels_frame.iloc[idx, :].values.astype('float32')\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        return features, label\n",
    "\n",
    "\n",
    "features_file = 'AwA2-features-float.csv'\n",
    "labels_file = 'AwA2-labels.csv'\n",
    "custom_dataset = CustomDataset(features_file, labels_file,0.5)\n",
    "# Split the dataset into train and test sets\n",
    "train_size = 0.8\n",
    "test_size = 1 - train_size\n",
    "train_dataset, test_dataset = train_test_split(custom_dataset, test_size=test_size, random_state=42)\n",
    "\n",
    "# Create data loaders\n",
    "awa_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "awa_test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# define the NN architecture\n",
    "class AWA_Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(AWA_Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2048,512),nn.ReLU(),nn.Linear(512,128),nn.ReLU(),nn.Linear(128, encoding_dim),nn.Sigmoid())\n",
    "        ## decoder ##\n",
    "        self.decoder = nn.Sequential(nn.Linear(encoding_dim, 128),nn.ReLU(),nn.Linear(128, 512),nn.ReLU(),nn.Linear(512,2048),nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define feedforward behavior \n",
    "        # and scale the *output* layer with a sigmoid activation function\n",
    "        \n",
    "        # pass x into encoder\n",
    "        out = (self.encoder(x))\n",
    "        # pass out into decoder\n",
    "        out = (self.decoder(out))\n",
    "        \n",
    "        return out\n",
    "    def embed(self,x):\n",
    "        return (self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWA_Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=2048, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=85, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=85, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# AWA model\n",
    "encoding_dim = 85\n",
    "awa_model = AWA_Autoencoder(encoding_dim)\n",
    "print(awa_model)\n",
    "\n",
    "# specify loss function\n",
    "criterionmse = nn.MSELoss()\n",
    "criterionce = nn.CrossEntropyLoss()\n",
    "criterionmce = nn.BCEWithLogitsLoss()\n",
    "# Loss function - ELBO (Evidence Lower Bound) with MSE loss\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')#alculate MSE loss\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(awa_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_function() missing 4 required positional arguments: 'recon_x', 'x', 'mu', and 'log_var'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m latent \u001b[39m=\u001b[39m awa_model\u001b[39m.\u001b[39membed(images)\n\u001b[1;32m     23\u001b[0m \u001b[39m# calculate the loss\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m loss \u001b[39m=\u001b[39m loss_function()\n\u001b[1;32m     25\u001b[0m \u001b[39m# perform a single optimization step (parameter update)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_function() missing 4 required positional arguments: 'recon_x', 'x', 'mu', and 'log_var'"
     ]
    }
   ],
   "source": [
    "#AWA\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0.0\n",
    "    for data in awa_train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, labels = data\n",
    "        \n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1)\n",
    "\n",
    "        attr = [predicate_matrix[int(x[0])-1] for x in labels]\n",
    "        attr = torch.tensor(attr)\n",
    "        attr = attr.float()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = awa_model(images)\n",
    "        latent = awa_model.embed(images)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterionmse(outputs, images)\n",
    "        loss+= 0.1*criterionmce(latent,attr)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(awa_train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error: 0.1827\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "num_samples = 0\n",
    "for data in awa_test_loader:\n",
    "    img, _ = data\n",
    "    img = img.view(img.size(0), -1)\n",
    "    output = awa_model(img)\n",
    "    loss = criterionmse(output, img)\n",
    "    total_loss += loss.item() * img.size(0)\n",
    "    num_samples += img.size(0)\n",
    "\n",
    "average_loss = total_loss / num_samples\n",
    "print(f'Average Reconstruction Error: {average_loss:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing nearest neighbour search for the predicted attribute vector to find the Zero Shot image label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pick the label given the attribute vector\n",
    "def zsl_label_prediction(predicted_vector):\n",
    "    predicted_vector_np = predicted_vector.detach().numpy()  # Convert from PyTorch tensor to NumPy array\n",
    "    #predicted_vector_np = np.array(predicted_vector)  # Convert from PyTorch tensor to NumPy array\n",
    "    # Calculate the cosine similarity between the predicted vector and each binary vector\n",
    "    similarities = []\n",
    "    for binary_vector in predicate_matrix:\n",
    "        binary_vector_np = np.array(binary_vector) # Convert from PyTorch tensor to NumPy array\n",
    "        similarity = np.dot(predicted_vector_np, binary_vector_np) / (np.linalg.norm(predicted_vector_np) * np.linalg.norm(binary_vector_np))\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Find the index of the binary vector with the highest similarity\n",
    "    closest_index = np.argmax(similarities)\n",
    "\n",
    "    # Retrieve the closest binary vector\n",
    "    return closest_index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "[13.]\n"
     ]
    }
   ],
   "source": [
    "image, label = test_dataset[35]  # Sample run on an image\n",
    "image = torch.tensor(image)\n",
    "with torch.no_grad():  # No need to compute gradients during inference\n",
    "    output = awa_model.embed(image.unsqueeze(0))  # Adding batch dimension since model expects batch input\n",
    "\n",
    "#output = awa_model.forward(image)\n",
    "predicted_attr = torch.softmax(output, dim=1)\n",
    "\n",
    "pred = zsl_label_prediction(predicted_attr)\n",
    "print(pred)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31422448]\n"
     ]
    }
   ],
   "source": [
    "#Evaluating ZSL on the test data set\n",
    "awa_model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    # Get the i-th sample from the test dataset\n",
    "    sample = test_dataset[i]\n",
    "\n",
    "    # Extract input and label from the sample\n",
    "    input_data, label = sample\n",
    "\n",
    "    input_data = torch.tensor(input_data)  # Convert to tensor if necessary\n",
    "    # input_data = input_data.to(device)  # Move to device if necessary\n",
    "\n",
    "    # Perform a forward pass through the model to obtain predictions\n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        output = awa_model.embed(input_data.unsqueeze(0))  # Assuming model expects a batch dimension\n",
    "    predicted_attr = torch.softmax(output, dim=1)\n",
    "    pred = zsl_label_prediction(predicted_attr)\n",
    "    correct+= pred==label\n",
    "    total+=1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
